{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a303992-453b-423e-ab24-c0e49bcfeed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()  # Automatically finds and sets SPARK_HOME\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(\"Spark version:\", spark.version)\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b31cc6-358e-409b-928e-b4ac6b4a0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+--------------------+--------------+------+-----------+------+------+--------------------+-----------------+-----+-----+-------+------------------+--------+--------------------+----------+--------------------+----------+------------------+------------------+--------+\n",
      "|trans_date_trans_time|          cc_num|            merchant|      category|   amt|      first|  last|gender|              street|             city|state|  zip|    lat|              long|city_pop|                 job|       dob|           trans_num| unix_time|         merch_lat|        merch_long|is_fraud|\n",
      "+---------------------+----------------+--------------------+--------------+------+-----------+------+------+--------------------+-----------------+-----+-----+-------+------------------+--------+--------------------+----------+--------------------+----------+------------------+------------------+--------+\n",
      "|  2019-08-11 19:38:33|4681601008538160|fraud_Hermann and...|  shopping_pos| 50.81|      Tammy| Davis|     F|77663 Colleen Fre...|      Moundsville|   WV|26041|39.9148|-80.73100000000001|   16183|     Physiotherapist|1977-08-12|f64003a5726ca9951...|1344713913|         39.238053|        -81.728984|       0|\n",
      "|  2020-02-21 13:31:42|4653879239169997|fraud_Cronin, Ksh...|health_fitness|  1.97|     Monica|Tucker|     F|302 Christina Isl...|     Smiths Grove|   KY|42171|37.0581|          -86.1938|    6841|   Therapist, sports|1999-06-06|f8e25094098b22a8e...|1361453502|         36.665575|        -85.608885|       0|\n",
      "|  2019-09-03 02:39:49|4334230547694630|fraud_Rutherford-...|   grocery_pos|100.18|      Scott|Martin|     M|  7483 Navarro Flats|          Freedom|   WY|83120|43.0172|         -111.0292|     471|Education officer...|1967-08-02|291160a410748e5be...|1346639989|43.362578000000006|       -110.580382|       0|\n",
      "|  2020-03-31 02:39:04|2256234701263057|  fraud_Kris-Padberg|  shopping_pos|  7.67|Christopher|Grimes|     M|   39631 Dunn Radial|         Grantham|   NH| 3753|43.5103|          -72.1334|    2971|  Charity fundraiser|1948-04-11|df716fe9ce97522bc...|1364697544|         44.312773|        -71.555079|       0|\n",
      "|  2020-12-05 07:58:57|6011948324228984|fraud_Towne, Gree...|  shopping_net| 478.1|   Mckenzie|Brooks|     F| 3872 Matthew Skyway|            Paris|   MS|38949|34.1992|           -89.382|     297|Administrator, lo...|1961-12-14|c33bcd8c63d50b4fe...|1386230337|34.362314000000005|        -89.623306|       0|\n",
      "|  2020-10-19 01:22:18|  30026790933302|fraud_Herman, Tre...|      misc_net|107.49|       John|Peters|     M|   555 Michael Burgs|      Mayersville|   MS|39113|32.9013|          -91.0286|     595|    Technical brewer|1979-09-03|976e58ac18356b393...|1382145738|         31.928995|        -91.321494|       0|\n",
      "|  2019-04-19 11:42:53|  30551643947183|      fraud_Cole PLC|   grocery_pos|231.61|     Morgan| Smith|     F|  1441 Bradley Place|           Grover|   NC|28073|35.1836|          -81.4552|    5621|        Toxicologist|1973-11-14|5ba428ae126192628...|1334835773|         35.257566|        -81.134489|       0|\n",
      "|  2019-09-23 11:08:42|3559160581764413|    fraud_Schumm PLC|  shopping_net| 85.74|      Allen|  Bell|     M| 70147 Amanda Fields|Saint Bonaventure|   NY|14778|42.2701|          -78.6847|    1453|        Toxicologist|1974-02-15|220c69d2e09d3323b...|1348398522|         42.583145|        -78.244389|       0|\n",
      "|  2019-02-21 13:38:55|3583635130604947|fraud_Weber, Thie...|     kids_pets|146.38|    Crystal|Gamble|     F|899 Michele View ...|     Philadelphia|   PA|19149|40.0369|          -75.0664| 1526206| Structural engineer|1985-01-01|a569367f8df5d5d34...|1329831535|         40.610395|-74.62679399999999|       0|\n",
      "|  2019-06-22 02:19:22|6564459919350820|fraud_Towne, Gree...|  shopping_net|  1.46|    Douglas|Willis|     M|619 Jeremy Garden...|           Benton|   WI|53803|42.5545|          -90.3508|    1306|Public relations ...|1958-09-10|9b13b6d6222b43f14...|1340331562|         42.845955|        -90.682929|       0|\n",
      "+---------------------+----------------+--------------------+--------------+------+-----------+------+------+--------------------+-----------------+-----+-----+-------+------------------+--------+--------------------+----------+--------------------+----------+------------------+------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('final_data.csv',header=True,inferSchema=True,mode=\"DROPMALFORMED\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28c54c1-6045-4c8c-86e8-8c38c10cd8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be062e5-896b-47d0-b066-539844d18f59",
   "metadata": {},
   "source": [
    "# 1. What are the top 5 cities where most fraudulent transactions occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca277619-7ae0-4089-a54f-2f9d77acc9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         city|count|\n",
      "+-------------+-----+\n",
      "|       Dallas|   39|\n",
      "|      Houston|   39|\n",
      "|   Birmingham|   36|\n",
      "|New York City|   35|\n",
      "|    Allentown|   34|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.is_fraud == 1).groupBy(\"city\").count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5c074-8d48-4430-8180-d05b673b6e65",
   "metadata": {},
   "source": [
    "# 2.What are the most frequent transaction categories associated with fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43938a69-e418-45a4-bfc1-3a5f9c3a8330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      category|count|\n",
      "+--------------+-----+\n",
      "|   grocery_pos| 2228|\n",
      "|  shopping_net| 2219|\n",
      "|      misc_net| 1182|\n",
      "|  shopping_pos| 1056|\n",
      "| gas_transport|  772|\n",
      "|      misc_pos|  322|\n",
      "|     kids_pets|  304|\n",
      "| entertainment|  292|\n",
      "| personal_care|  290|\n",
      "|          home|  265|\n",
      "|   food_dining|  205|\n",
      "|health_fitness|  185|\n",
      "|   grocery_net|  175|\n",
      "|        travel|  156|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.is_fraud == 1).groupBy('category').count().orderBy(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815c2d9-cf85-4319-abc0-0316efde2602",
   "metadata": {},
   "source": [
    "# 3. What are top 10 most frequent merchants for fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d205ee-3930-4a3f-ab33-2f5ea2c428f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            merchant|count|\n",
      "+--------------------+-----+\n",
      "|   fraud_Kilback LLC|   62|\n",
      "|  fraud_Rau and Sons|   60|\n",
      "|   fraud_Kozey-Boehm|   60|\n",
      "|     fraud_Doyle Ltd|   57|\n",
      "|    fraud_Terry-Huel|   56|\n",
      "|      fraud_Kuhn LLC|   55|\n",
      "|     fraud_Boyer PLC|   55|\n",
      "|     fraud_Kuhic LLC|   53|\n",
      "|fraud_Moen, Reing...|   53|\n",
      "|fraud_Kiehn-Emmerich|   53|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.is_fraud == 1).groupBy(\"merchant\").count().orderBy(\"count\",ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eded7b6-84c4-49cc-8ae2-3276a4f768fc",
   "metadata": {},
   "source": [
    "# 4. Calculate the avg amount spend by each gender and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be03be01-c9f8-45ab-8b16-c8f387a39334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|gender|          avg(amt)|\n",
      "+------+------------------+\n",
      "|     F|271.71298766157497|\n",
      "|     M|330.50721562156156|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.groupBy(\"gender\").mean(\"amt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a96f16-2abd-4a8e-9e5e-041f1f2b1189",
   "metadata": {},
   "source": [
    "# 5. Top 5 merchant with highest average transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73df3295-9661-47fd-b16c-08277d21ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|            merchant|         mean_amt|\n",
      "+--------------------+-----------------+\n",
      "|fraud_Bashirian G...|891.0655102040814|\n",
      "|     fraud_Kuhic LLC|869.5223437500001|\n",
      "|fraud_Schmidt and...|852.2311111111113|\n",
      "|fraud_Heathcote, ...|838.5803225806453|\n",
      "|   fraud_Kozey-Boehm| 832.279864864865|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.groupBy(\"merchant\").agg(mean(\"amt\").alias(\"mean_amt\")).orderBy(\"mean_amt\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20454bb4-4a79-43a9-a10e-5122f4eb65a0",
   "metadata": {},
   "source": [
    "# 6. Calculate the age of each customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e62949ea-5237-4f48-997b-77fec637e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---+\n",
      "|    first|     last|Age|\n",
      "+---------+---------+---+\n",
      "|   Robert|   Haynes| 28|\n",
      "|  Vincent|   Waller| 71|\n",
      "|     Anna|    Logan| 30|\n",
      "|   Melvin|   Wright| 24|\n",
      "|    James|  Baldwin| 45|\n",
      "|Stephanie|    Crane| 70|\n",
      "|     Jodi|Rodriguez| 48|\n",
      "|     Adam| Mcdonald| 34|\n",
      "|  Katelyn|     Wise| 88|\n",
      "|    Brian| Williams| 38|\n",
      "|     Sara|   Harris| 49|\n",
      "|   Andrea|  Perkins| 53|\n",
      "| Kimberly|     Webb| 75|\n",
      "| Michelle|     Beck| 58|\n",
      "|    Scott|     Cole| 65|\n",
      "| Jennifer|    Black| 44|\n",
      "|  Michael|    Jones| 64|\n",
      "|  Bethany|  Andrade| 59|\n",
      "|  William|  Johnson| 68|\n",
      "|  Charles|  Preston| 63|\n",
      "+---------+---------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, to_timestamp\n",
    "from datetime import datetime\n",
    "\n",
    "df = df.withColumn(\"dob_timestamp\", to_timestamp(col(\"dob\")))\n",
    "\n",
    "current_year = datetime.now().year\n",
    "\n",
    "df1 = df.withColumn(\"Age\", current_year - year(col(\"dob_timestamp\")))\n",
    "\n",
    "df1.select(\"first\",\"last\", \"Age\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c294b-6eb5-4c6c-a2c6-db0d467754e6",
   "metadata": {},
   "source": [
    "# 7. Top 5 states with highest number of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957051a6-aa11-42c0-abdd-5a1b9ba9622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|   NY| 1290|\n",
      "|   TX| 1287|\n",
      "|   PA| 1153|\n",
      "|   CA|  857|\n",
      "|   OH|  687|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('state').count().orderBy('count',ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e8ec0-e882-47d1-ad00-734d0aaa26e8",
   "metadata": {},
   "source": [
    "# 8.  Distribution of transaction over the course of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c70477-5be9-4cbc-9a81-4c55f1135758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|    1| 1646|\n",
      "|    2| 1275|\n",
      "|    3| 1804|\n",
      "|    4| 1663|\n",
      "|    5| 1747|\n",
      "|    6| 1642|\n",
      "|    7| 1631|\n",
      "|    8| 1505|\n",
      "|    9| 1631|\n",
      "|   10| 1514|\n",
      "|   11| 1675|\n",
      "|   12| 1569|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, month, to_timestamp\n",
    "df = df.withColumn(\"dob_timestamp\", to_timestamp(col(\"dob\")))\n",
    "\n",
    "df = df.withColumn(\"month\", month(col(\"dob_timestamp\")))\n",
    "\n",
    "df.groupBy(\"month\").count().orderBy('month',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb116fc4-c533-4d7d-9044-ffa8e9a370f2",
   "metadata": {},
   "source": [
    "# 10. Analyse spending pattern based on age groups and avg spending amount for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2127117-5caa-4f11-93cd-b50aef69b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|age_group|         avg_spent|\n",
      "+---------+------------------+\n",
      "|    18-25| 341.1335061728393|\n",
      "|    26-35|  300.274189364462|\n",
      "|    36-45| 256.7721430363867|\n",
      "|    46-55|244.01207727044644|\n",
      "|    56-65|354.12696461824964|\n",
      "|      65+| 341.5525411255414|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, to_timestamp\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, when, avg\n",
    "\n",
    "\n",
    "df = df.withColumn(\"dob_timestamp\", to_timestamp(col(\"dob\")))\n",
    "\n",
    "current_year = datetime.now().year\n",
    "\n",
    "df1 = df.withColumn(\"age\", current_year - year(col(\"dob_timestamp\")))\n",
    "\n",
    "\n",
    "df1 = df1.withColumn(\"age_group\", \n",
    "    when(col(\"age\").between(18, 25), \"18-25\")\n",
    "    .when(col(\"age\").between(26, 35), \"26-35\")\n",
    "    .when(col(\"age\").between(36, 45), \"36-45\")\n",
    "    .when(col(\"age\").between(46, 55), \"46-55\")\n",
    "    .when(col(\"age\").between(56, 65), \"56-65\")\n",
    "    .when(col(\"age\") > 65, \"65+\")\n",
    ")\n",
    "\n",
    "\n",
    "avg_spent = df1.groupBy(\"age_group\").agg(avg(\"amt\").alias(\"avg_spent\")).orderBy(\"age_group\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4728c-b78a-45c9-855b-8c9feea8e6e9",
   "metadata": {},
   "source": [
    "# Python libraries and functions from PySpark (pyspark.sql.functions)\n",
    "# ----------------------------------------------------------------------------\n",
    "# col- references DataFrame columns.\n",
    "# year - extracts the year.\n",
    "# to_timestamp - converts a string to a timestamp.\n",
    "# month - extracts the month.\n",
    "# datetime - provides date/time functions.\n",
    "# when - applies conditional logic.\n",
    "# avg - calculates the average.\n",
    "# .withColumn() - to add new columns based on existing ones.\n",
    "# agg() - applies multiple aggregate functions (like avg(), sum(), etc.) to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76fe87-3330-4e4d-b1c5-a1da155dac9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
